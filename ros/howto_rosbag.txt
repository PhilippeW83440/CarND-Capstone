Some notes on testing and checking results with a reference rosbag file provided by Udacity for a test in Udacity parking lot:


DBW test howto:
---------------
1) download https://drive.google.com/open?id=0B2_h37bMVw3iT0ZEdlF4N01QbHc4
   you get a file: udacity_succesful_light_detection.bag
2) rename the file to dbw_test.rosbag.bag
3) move the dbw_test.rosbag.bag file to the project rootâ€™s data folder, e.g. CarND-Capstone/data/dbw_test.rosbag.bag
4) Run dbw_test: roslaunch src/twist_controller/launch/dbw_test.launch
5) launch rviz in another terminal just to visualize camera and lidar sensors 
When test is done:
6) Compare your results to the dbw_test output logged in ros/src/twist_controller in three files: brakes.csv, steers.csv, throttles.csv


Carla vs simulation specificities:
----------------------------------
We are at very low speed: < 10 km/h (i.e. < 2.77 m/s)
1) throttle is at max 0.025 (instead of 1 potentially at regular speeds)
2) brake is not in Nm (mass * accel * wheel_radius) but in (accel * wheel_radius) apparently ... 
   Use a percentage mode: percentage vs mass
   cf: https://github.com/udacity/sdc-issue-reports/issues/1204
3) steer is (or can be just computed as) proposed_angular_velocity (as sent by pure pursuit) * steer_ratio ... that is all ...
   It results in more agressive steering which is OK and even usefull at such low speed
With these modifications we have a very good match match while running dbw_test.py on dbw_test.rosbag.bag

TL test howto:
--------------
1) roslaunch launch/site.launch
2) rosbag play udacity_succesful_light_detection.bag
3) check all images in: ros/src/tl_detector/light_classification/debug/imageXXX.png

GPU WARNING: 
- Carla is using a powerfull GPU Titan X
- Code was tested with a GTX 1080 TI and GTX 980 TI
- we are running faster_rcnn for TL detection every 333 ms: so your GPU should run faster_rcnn on an image in less than 333 ms 
  (minus time for other nodes processing and minus time for simulator graphics if you are using it ...)
